{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T20:23:57.571411Z",
     "start_time": "2025-10-10T20:23:10.175279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Essentially multiple instances of causal attention running independently with their own weights for queries keys and values\n",
    "# we will therefore get multiple resulting context vectors that we will concatenate in the end, so rows stay the same, n input tokens, and n output context vectors but the columns increase\n",
    "\n",
    "# computationally expensive but performance gain is significant as for every different matrix another multiplication needs to be done which increase linearly, so we can instead generate with 3 big weight matrices in the start, each for query key and values, and then we just do 1 multiplication and split the results into parts\n",
    "\n",
    "from nltk import word_tokenize\n",
    "import gensim.downloader as api\n",
    "import torch\n",
    "\n",
    "glove_embeddings = api.load(\"glove-twitter-25\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_device('cuda')\n",
    "device"
   ],
   "id": "db9c0799d2c72480",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T20:23:58.355087Z",
     "start_time": "2025-10-10T20:23:57.650623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentence = \"Your journey starts with one step Your journey starts with one step Your journey starts with one step starts with\"\n",
    "tokens = word_tokenize(sentence.lower())\n",
    "encoded = torch.tensor(glove_embeddings[tokens])\n",
    "# let num tokens be n\n",
    "encoded.shape"
   ],
   "id": "3ee4b10e653caf55",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 25])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T20:23:59.009309Z",
     "start_time": "2025-10-10T20:23:58.384269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    # num of embeddings are n, n at max can be equal to context length\n",
    "    def __init__(self, embed_dim, context_length, output_dim, num_heads, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.context_length = context_length\n",
    "\n",
    "        # trainable layers for initializing queries, keys and values\n",
    "        self.w_queries = torch.nn.Linear(embed_dim, output_dim*num_heads, bias=False) # embed_dim x output_dim*num_heads\n",
    "        self.w_keys = torch.nn.Linear(embed_dim, output_dim*num_heads, bias=False)\n",
    "        self.w_values = torch.nn.Linear(embed_dim, output_dim*num_heads, bias=False)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = embeddings[: self.context_length * (len(embeddings)//self.context_length)] # n x embed_dim\n",
    "        embeddings = torch.reshape(embeddings, (-1, self.context_length, self.embed_dim))  # batches x context_len x embed_dim\n",
    "        batches = len(embeddings)\n",
    "\n",
    "        all_queries = self.w_queries(embeddings) # batches x context_len x output_dim*num_heads\n",
    "        all_keys = self.w_keys(embeddings)\n",
    "        all_values = self.w_values(embeddings)\n",
    "\n",
    "        # split them in columns and then line them up in a tensor\n",
    "        queries =  all_queries.view(batches, self.num_heads, self.context_length, self.output_dim)\n",
    "        keys = all_keys.view(batches, self.num_heads, self.context_length, self.output_dim)\n",
    "        values = all_values.view(batches, self.num_heads, self.context_length, self.output_dim)\n",
    "\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(2,3) # numheads x n x n\n",
    "\n",
    "        # as this is causal attention now we will mask the upper right diagonal\n",
    "        causal_mask_bool =  torch.triu(torch.ones_like(attention_scores), diagonal=1).bool() #triu stands for triangle up\n",
    "\n",
    "        attention_scores.masked_fill_(causal_mask_bool, -torch.inf) # now a word only depend on the words before it, as the future dependencies are -ve infinity so after softmax the probabilities will be zero\n",
    "\n",
    "        attention_weights = torch.softmax(attention_scores / self.output_dim**0.5, dim=2) # batches x numheads x context_len x context_len\n",
    "\n",
    "        context_vectors = (attention_weights @ values).transpose(1, 2)\n",
    "        # batches x context x numheads x output_dim <from> batches x numheads x context x output_dim\n",
    "        context_vector = context_vectors.contiguous().view(batches*context_len, self.output_dim*self.num_heads)\n",
    "        # batches x context x numheads*output_dim\n",
    "        return context_vector\n",
    "\n",
    "context_len = 4\n",
    "out_dim = 3\n",
    "heads = 2\n",
    "\n",
    "attention_head = MultiHeadAttention(encoded.shape[-1], context_len, out_dim, heads, batch_size=6)\n",
    "context = attention_head.forward(encoded)\n",
    "encoded.shape , context.shape"
   ],
   "id": "e604e78fa0009627",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 25]), torch.Size([20, 6]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
