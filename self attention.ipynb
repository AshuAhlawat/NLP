{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-28T20:01:29.093738Z",
     "start_time": "2025-09-28T20:01:29.089569Z"
    }
   },
   "source": [
    "from nltk import word_tokenize\n",
    "import gensim.downloader as api\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T20:01:59.362222Z",
     "start_time": "2025-09-28T20:01:29.103672Z"
    }
   },
   "cell_type": "code",
   "source": "model = api.load(\"glove-twitter-25\")",
   "id": "845297d4bcae2992",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T20:01:59.392791Z",
     "start_time": "2025-09-28T20:01:59.374103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentence = \"Your journey starts with one step\"\n",
    "tokens = word_tokenize(sentence.lower())\n",
    "tokens"
   ],
   "id": "53ff378af21f3d3b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['your', 'journey', 'starts', 'with', 'one', 'step']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T20:01:59.454710Z",
     "start_time": "2025-09-28T20:01:59.436369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embeddings = torch.tensor(model[tokens])\n",
    "embeddings.shape"
   ],
   "id": "25f12730bb64222e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 25])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T20:03:42.594523Z",
     "start_time": "2025-09-28T20:03:42.579339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "d_in = embeddings.shape[-1]\n",
    "d_out = 2\n",
    "\n",
    "w_queries = torch.nn.Linear(d_in,d_out, bias=False)\n",
    "w_keys = torch.nn.Linear(d_in,d_out, bias=False)\n",
    "w_values = torch.nn.Linear(d_in,d_out, bias=False)\n",
    "w_values.weight.shape"
   ],
   "id": "dad9c7d22ee8289d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 25])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T20:06:14.312915Z",
     "start_time": "2025-09-28T20:06:14.300207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# the following variable will have meaning when we train the w matrices using NN's\n",
    "# this is essentially just matrix multiplication of (2,25) weights with (,25) embeddings\n",
    "queries = w_queries(embeddings) # represents the current token the model is focusing on\n",
    "keys = w_keys(embeddings) # keys are like token embeddings but specialized to this sentence, so we can judge the relation between them/ match with queries\n",
    "values = w_values(embeddings)# reduced dimension representation of a token\n",
    "values.shape"
   ],
   "id": "611458191de436fd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T20:06:17.123544Z",
     "start_time": "2025-09-28T20:06:17.085456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# SCALED DOT PRODUCT ATTENTION\n",
    "\n",
    "attention_scores = queries @ keys.T\n",
    "# essentially a matrix of what percent every query is affected by every other token in the sentence\n",
    "attention_weights = torch.softmax(attention_scores / d_out**0.5, dim=1)\n",
    "# we will scale the above by root of d_out, which makes the softmax less strict the higher the number of d_out, making the output more stable, as the higher the d_out the higher the variance of the distribution, so to keep it close to 1 we divide\n",
    "print(attention_weights)"
   ],
   "id": "b2b808cb86972940",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1770, 0.1408, 0.1560, 0.1981, 0.1814, 0.1466],\n",
      "        [0.1430, 0.1570, 0.2047, 0.1667, 0.1642, 0.1645],\n",
      "        [0.1701, 0.1706, 0.1612, 0.1639, 0.1656, 0.1686],\n",
      "        [0.1669, 0.1487, 0.1690, 0.1857, 0.1754, 0.1542],\n",
      "        [0.1866, 0.1420, 0.1442, 0.1991, 0.1825, 0.1457],\n",
      "        [0.1508, 0.1555, 0.1921, 0.1720, 0.1676, 0.1620]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T20:06:25.503167Z",
     "start_time": "2025-09-28T20:06:25.488533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# now we assign the weightages of all the keys to the values\n",
    "context_vectors = attention_weights @ values\n",
    "# enriched vectors which has the semantic meaning of a token wrt all other tokens that affect it\n",
    "context_vectors.shape"
   ],
   "id": "11704a5cf832e7f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CAUSAL ATTENTION ( masked attention )\n",
    "# variation of self attention in which we only have access to the current and the previous tokens\n",
    "# code is also same as self attention but the context vectors are of the form\n",
    "# x 0 0 0\n",
    "# x x 0 0\n",
    "# x x x 0\n",
    "# x x x x\n",
    "# using an attention mask\n",
    "# instead of\n",
    "# x x x x\n",
    "# x x x x\n",
    "# x x x x\n",
    "# x x x x"
   ],
   "id": "961cecaec9b0d093"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
